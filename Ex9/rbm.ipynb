{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self, num_visible, num_hidden, learning_rate=0.1, epochs=100):\n",
    "        self.num_visible = num_visible\n",
    "        self.num_hidden = num_hidden\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.visible_bias = tf.Variable(tf.random.normal([self.num_visible]), name=\"visible_bias\")\n",
    "        self.hidden_bias = tf.Variable(tf.random.normal([self.num_hidden]), name=\"hidden_bias\")\n",
    "        self.weights = tf.Variable(tf.random.normal([self.num_visible, self.num_hidden], stddev=0.1), name=\"weights\")\n",
    "\n",
    "    def sample_hidden(self, visible_samples):\n",
    "        hidden_activation = tf.nn.sigmoid(tf.matmul(visible_samples, self.weights) + self.hidden_bias)\n",
    "        hidden_samples = tf.nn.relu(tf.sign(hidden_activation - tf.random.uniform(tf.shape(hidden_activation))))\n",
    "        return hidden_samples\n",
    "\n",
    "    def sample_visible(self, hidden_samples):\n",
    "        visible_activation = tf.nn.sigmoid(tf.matmul(hidden_samples, tf.transpose(self.weights)) + self.visible_bias)\n",
    "        visible_samples = tf.nn.relu(tf.sign(visible_activation - tf.random.uniform(tf.shape(visible_activation))))\n",
    "        return visible_samples\n",
    "\n",
    "    def train(self, data):\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch in data:\n",
    "                visible_samples = tf.convert_to_tensor(batch, dtype=tf.float32)\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    hidden_samples = self.sample_hidden(visible_samples)\n",
    "                    reconstructed_visible_samples = self.sample_visible(hidden_samples)\n",
    "                    positive_gradient = tf.matmul(tf.transpose(visible_samples), hidden_samples)\n",
    "                    negative_gradient = tf.matmul(tf.transpose(reconstructed_visible_samples), self.sample_hidden(reconstructed_visible_samples))\n",
    "\n",
    "                    contrastive_divergence = (positive_gradient - negative_gradient) / tf.cast(tf.shape(visible_samples)[0], tf.float32)\n",
    "                    self.weights.assign_add(self.learning_rate * contrastive_divergence)\n",
    "                    self.visible_bias.assign_add(self.learning_rate * tf.reduce_mean(visible_samples - reconstructed_visible_samples, axis=0))\n",
    "                    self.hidden_bias.assign_add(self.learning_rate * tf.reduce_mean(hidden_samples - self.sample_hidden(reconstructed_visible_samples), axis=0))\n",
    "\n",
    "    def reconstruct(self, data):\n",
    "        visible_samples = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "        hidden_samples = self.sample_hidden(visible_samples)\n",
    "        reconstructed_visible_samples = self.sample_visible(hidden_samples)\n",
    "        return reconstructed_visible_samples.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Samples:\n",
      "[[1. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 0. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [1. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [1. 1. 0. 0. 1.]\n",
      " [1. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 1.]\n",
      " [0. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [1. 0. 1. 1. 1.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [1. 0. 1. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 1. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "np.random.seed(123)\n",
    "data = np.random.randint(0, 2, size=(100, 5))  # Sample binary data\n",
    "rbm = RBM(num_visible=5, num_hidden=3, learning_rate=0.1, epochs=1000)\n",
    "rbm.train([data])  # Note: Wrap data in a list to create a batch\n",
    "reconstructed_samples = rbm.reconstruct(data)\n",
    "print(\"Reconstructed Samples:\")\n",
    "print(reconstructed_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
